{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ChimeraLC/CHIP-8-emulator/blob/main/DANN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#DANN"
      ],
      "metadata": {
        "id": "dQoBo1rXkSeH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preparation"
      ],
      "metadata": {
        "id": "UONQiUIlkPlu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "5wj6YzcEo2Y0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1280c956-3220-434f-d874-28bd029cf085"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Preparation.\n",
        "import csv\n",
        "import itertools\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "from collections import defaultdict\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Function\n",
        "import torch.utils.data as data\n",
        "from torch.utils.data import DataLoader\n",
        "from sklearn.metrics import roc_auc_score, roc_curve, average_precision_score, precision_recall_curve, accuracy_score\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "EdtDhAKMo_ei"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "criteo_train_data_x = pd.read_csv('/content/drive/MyDrive/DADF/Criteo/Criteo_train_x.csv')\n",
        "criteo_train_data_y = pd.read_csv('/content/drive/MyDrive/DADF/Criteo/Criteo_train_y.csv')\n",
        "criteo_test_data_x = pd.read_csv('/content/drive/MyDrive/DADF/Criteo/Criteo_test_x.csv')\n",
        "criteo_test_data_y = pd.read_csv('/content/drive/MyDrive/DADF/Criteo/Criteo_test_y.csv')\n",
        "total_data = pd.concat([criteo_train_data_x,criteo_test_data_x])"
      ],
      "metadata": {
        "id": "0rG4ycY-yD2K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the min value and max value of each integer columns.\n",
        "integer_min_max = {'i' + str(i): [total_data['i' + str(i)].min(), total_data['i' + str(i)].max()] for i in range(1, 9)}\n",
        "# Apply min max normalization for each integer value column\n",
        "for i in range(1,9):\n",
        "  col = 'i' + str(i)\n",
        "  if col in integer_min_max:\n",
        "      min_val, max_val = integer_min_max[col]\n",
        "      criteo_train_data_x[col] = criteo_train_data_x[col].apply(lambda x: (x - min_val) / (max_val - min_val))\n",
        "      criteo_test_data_x[col] = criteo_test_data_x[col].apply(lambda x: (x - min_val) / (max_val - min_val))"
      ],
      "metadata": {
        "id": "X-J6lQK3Y6Dl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get unique values for all categorical value columns.\n",
        "cate_dict = defaultdict(list)\n",
        "for i in range(1,10):\n",
        "    col = 'c' + str(i)\n",
        "    unique_val = list(total_data[col].unique())\n",
        "    cate_dict[col] = unique_val"
      ],
      "metadata": {
        "id": "qv-Eu6MEAYEi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Encoding for categorical values.\n",
        "def create_empty_array():\n",
        "    return np.array([])\n",
        "# Each unique value will be encoded as starting from 1, 2, 3, .....\n",
        "def get_ohe(col_name, uni_vals):\n",
        "    ohedict = defaultdict(create_empty_array)\n",
        "    i = 0\n",
        "    for val in uni_vals:\n",
        "        ohe_vec = i\n",
        "        ohedict[val] = ohe_vec\n",
        "        i += 1\n",
        "    return ohedict\n",
        "\n",
        "# Apply encoding for all Categorical value columns.\n",
        "all_cate = defaultdict(create_empty_array)\n",
        "for i in range(1,10):\n",
        "  col = 'c' + str(i)\n",
        "  all_cate[col] = get_ohe(col, cate_dict[col])\n",
        "\n",
        "for i in range(1,10):\n",
        "  col = 'c' + str(i)\n",
        "  val_dict = all_cate[col]\n",
        "  criteo_train_data_x[col] = criteo_train_data_x[col].apply(lambda x: val_dict[x])\n",
        "  criteo_test_data_x[col] = criteo_test_data_x[col].apply(lambda x: val_dict[x])"
      ],
      "metadata": {
        "id": "J-rpTIOoyD9y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preparing Source,target train data.\n",
        "seconds_a_day = 60*60*24\n",
        "# First 32 Days for training with label and the rest 28 days for training without label.\n",
        "source_train_indices = criteo_train_data_x[criteo_train_data_x['timestamp'] // seconds_a_day < 31].index\n",
        "target_train_indices = criteo_train_data_x[criteo_train_data_x['timestamp'] // seconds_a_day >= 31].index\n",
        "\n",
        "# Drop not used columns.\n",
        "criteo_source_train_data_x = np.array(criteo_train_data_x.loc[source_train_indices].drop(['convertTimestamp','timestamp'],axis=1))\n",
        "criteo_source_train_data_y = np.array(criteo_train_data_y.loc[source_train_indices])\n",
        "source_train_data_convertTime = np.array(criteo_train_data_x.loc[source_train_indices]['convertTimestamp'])\n",
        "\n",
        "cirteo_target_train_data_x = np.array(criteo_train_data_x.loc[target_train_indices].drop(['convertTimestamp','timestamp'],axis=1))\n",
        "criteo_target_train_data_y = np.array(criteo_train_data_y.loc[target_train_indices])\n",
        "target_train_data_convertTime = np.array(criteo_train_data_x.loc[target_train_indices]['convertTimestamp'])\n",
        "\n",
        "# Preparing test data.\n",
        "test_data_convertTime = np.array(criteo_test_data_x['convertTimestamp'])\n",
        "criteo_test_data_x = np.array(criteo_test_data_x.drop(['convertTimestamp','timestamp'],axis=1))\n",
        "criteo_test_data_y = np.array(criteo_test_data_y)"
      ],
      "metadata": {
        "id": "0umsscIGlSCv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Criteo Dataset.\n",
        "class Criteo_Dataset(data.Dataset):\n",
        "    def __init__(self, x, label, convertTimestamp):\n",
        "      self.numeric_x = x[:, :8]\n",
        "      self.cate_x = x[:, 8:]\n",
        "      self.true_label = label.copy()\n",
        "      self.domain_label = label.copy()\n",
        "      # Delayed feed back label, 60 and 61 day for testing(0 index).\n",
        "      condition = (convertTimestamp // 86400) > 59\n",
        "      self.domain_label[condition] = 0\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.true_label)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.numeric_x[idx], self.cate_x[idx], self.true_label[idx], self.domain_label[idx]"
      ],
      "metadata": {
        "id": "rkaY2kTkTp1w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DANN"
      ],
      "metadata": {
        "id": "icqkUtTckWV8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Reverse gradient layer.\n",
        "class ReverseLayerF(Function):\n",
        "    '''\n",
        "     Forward pass of the layer which simply returns the input as is.\n",
        "     * @param ctx   The context that stores information for backpropagation. Here, it stores the alpha value.\n",
        "     * @param x     The input tensor that will be passed through this layer.\n",
        "     * @param alpha The scalar to be used for scaling the gradient during backpropagation.\n",
        "     * @return      The input tensor itself (identity operation).\n",
        "    '''\n",
        "    @staticmethod\n",
        "    def forward(ctx, x, alpha):\n",
        "        ctx.alpha = alpha\n",
        "        return x.view_as(x)\n",
        "    '''\n",
        "     Backward pass of the layer where the gradients are scaled and the sign is reversed.\n",
        "     * @param ctx         The context where alpha was stored during the forward pass.\n",
        "     * @param grad_output The gradient tensor from the previous layer in the backpropagation.\n",
        "     * @return            The gradient tensor with its sign reversed and scaled by alpha.\n",
        "     '''\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output):\n",
        "        output = grad_output.neg() * ctx.alpha\n",
        "        return output, None"
      ],
      "metadata": {
        "id": "73_3lWBLZRkH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Initialize MLP.\n",
        "Args.\n",
        "@param input_size - The size of the input layer.\n",
        "@param hidden_sizes - The size of the hidden layers.\n",
        "@param output_size - The size of the output layer.\n",
        "@param predict - whether to generate predictions with sigmoid\n",
        "@param cate_size - list of categorical value dimension.\n",
        "@param embed_size - categorical value embedding dimension.\n",
        "@param dropout - The dropout value. Default is 0. ( 0 by default )\n",
        "@param normal - Whether to use batch Normalization (True by default)\n",
        "\"\"\"\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, input_size, hidden_sizes, output_size=0, predict=False, cate_size=None, embed_size=None, dropout=0, normal=True):\n",
        "        super(MLP, self).__init__()\n",
        "        self.has_embeddings = cate_size is not None and embed_size is not None\n",
        "        if self.has_embeddings:\n",
        "            self.embedlayers = nn.ModuleList([nn.Embedding(c, embed_size) for c in cate_size])\n",
        "            total_input_size = input_size + len(cate_size) * embed_size\n",
        "        else:\n",
        "            total_input_size = input_size\n",
        "        layers = []\n",
        "        for hidden_size in hidden_sizes:\n",
        "            layers.append(nn.Linear(total_input_size, hidden_size))\n",
        "            if normal:\n",
        "                layers.append(nn.BatchNorm1d(hidden_size))\n",
        "            layers.append(nn.ReLU())\n",
        "            if dropout > 0:\n",
        "                layers.append(nn.Dropout(dropout))\n",
        "            total_input_size = hidden_size\n",
        "\n",
        "        if output_size:\n",
        "          layers.append(nn.Linear(hidden_sizes[-1], output_size))\n",
        "\n",
        "        if predict:\n",
        "          layers.append(nn.Sigmoid())\n",
        "\n",
        "        self.mlp = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x, cate_x=None):\n",
        "        if self.has_embeddings and cate_x is not None:\n",
        "            embedded_cate_x = [embed(cate_x[:, i]) for i, embed in enumerate(self.embedlayers)]\n",
        "            x = torch.cat([x] + embedded_cate_x, dim=1)\n",
        "\n",
        "        return self.mlp(x)"
      ],
      "metadata": {
        "id": "iT93dQFY8T3c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Discriminator class wraps around MLP and apply reverse layer.\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, input_size, hidden_sizes, output_size):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.discriminator = MLP(input_size, hidden_sizes, output_size, predict=True)\n",
        "\n",
        "    def forward(self, x, alpha):\n",
        "        x = ReverseLayerF.apply(x, alpha)\n",
        "        return self.discriminator(x)"
      ],
      "metadata": {
        "id": "qpZFXbvl9lD3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# class DANN(nn.Module):\n",
        "#     def __init__(self, base_model, input_size, clf_hidden_sizes, feaex_hidden_sizes, dis_hidden_sizes, output_size, cate_size, embed_size, dropout):\n",
        "#         super(DANN, self).__init__()\n",
        "#         # Build Base model with MLP or MMoE.\n",
        "#         if base_model == 'MLP':\n",
        "#           # Feature Extractor.\n",
        "#           self.feature_extractor = MLP(input_size, feaex_hidden_sizes, cate_size=cate_size, embed_size=embed_size, dropout=dropout)\n",
        "#           # Classifier.\n",
        "#           fea_dim_input = feaex_hidden_sizes[-1]\n",
        "#           self.classifier = MLP(fea_dim_input, clf_hidden_sizes, output_size=output_size, predict=True, dropout=dropout)\n",
        "#           # Discriminator.\n",
        "#           self.discriminator = Discriminator(fea_dim_input, dis_hidden_sizes, output_size=output_size)\n",
        "#         else:\n",
        "\n",
        "\n",
        "#     def forward(self, x, cate_x=None, alpha=None):\n",
        "#         # Feature Extraction.\n",
        "#         features = self.feature_extractor(x, cate_x)\n",
        "#         # Classification.\n",
        "#         class_pred = self.classifier(features)\n",
        "#         # Discrimination.\n",
        "#         if alpha is not None:\n",
        "#             disc_pred = self.discriminator(features, alpha)\n",
        "#             return class_pred, disc_pred\n",
        "#         return class_pred\n"
      ],
      "metadata": {
        "id": "co436QxGy_Tr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluation"
      ],
      "metadata": {
        "id": "KqH_6er447M_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function for evaluating the model with accuracy, ROC_AUC and PR_AUC.\n",
        "def evaluate_model(pred, true):\n",
        "  acc_pred = torch.where(pred >= 0.5, 1, 0)\n",
        "  accuracy = accuracy_score(true.detach().cpu().numpy(), acc_pred.detach().cpu().numpy())\n",
        "  roc_auc = roc_auc_score(true.detach().cpu().numpy(), pred.detach().cpu().numpy())\n",
        "  pr_auc = average_precision_score(true.detach().cpu().numpy(), pred.detach().cpu().numpy())\n",
        "  return accuracy, roc_auc, pr_auc"
      ],
      "metadata": {
        "id": "QSDcA_UX49un"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Starting Training"
      ],
      "metadata": {
        "id": "y65tbXQ6kYdK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Scheduler for alpha being applied in the reverse gradient layer.\n",
        "def get_lambda(epoch, max_epoch):\n",
        "    p = epoch / max_epoch\n",
        "    return 2. / (1+np.exp(-10.*p)) - 1."
      ],
      "metadata": {
        "id": "s6F2YJKinsGL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(models, lrate, num_epoch, train_source_loader, train_target_loader, test_loader, model_directory):\n",
        "  # Extracting models.\n",
        "  clf, dis, feaex = models\n",
        "  clf.train()\n",
        "  dis.train()\n",
        "  feaex.train()\n",
        "  # Building Optimizers for each model.\n",
        "  clf_optimizer = torch.optim.Adam(clf.parameters(), lr=lrate)\n",
        "  dis_optimizer = torch.optim.Adam(dis.parameters(), lr=lrate)\n",
        "  feaex_optimizer = torch.optim.Adam(feaex.parameters(), lr=lrate)\n",
        "\n",
        "  clf_bce = nn.BCELoss()\n",
        "  dis_bce = nn.BCELoss()\n",
        "\n",
        "  test_loss_per_epoch = []; train_average_clf_loss_per_epoch = []; train_average_feaex_loss_per_epoch = [];prediction_loss_per_epoch = []\n",
        "  metric_accuracy = []; metric_roc_auc = []; metric_pr_auc = [];\n",
        "\n",
        "  # Train iteration.\n",
        "  for epoch in range(num_epoch):\n",
        "    target_iter = iter(itertools.cycle(train_target_loader))\n",
        "    one_epoch_clf_train_loss = []; one_epoch_feaex_train_loss = []\n",
        "    for i, (num_x, cate_x, true_y, delayed_y) in enumerate(train_source_loader):\n",
        "      target_num_x, target_cate_x, _, __ = next(target_iter)\n",
        "      num_x = num_x.to(dtype=torch.float32, device=device); cate_x = cate_x.to(dtype=torch.long, device=device); delayed_y = delayed_y.to(device); target_num_x = target_num_x.to(dtype=torch.float32, device=device); target_cate_x = target_cate_x.to(dtype=torch.long, device=device)\n",
        "      # Create Domain Label.\n",
        "      D_src = torch.ones(num_x.shape[0], 1).to(device)\n",
        "      D_tgt = torch.zeros(target_num_x.shape[0], 1).to(device)\n",
        "      # Extract features and Train discriminator.\n",
        "      alpha = get_lambda(epoch, num_epoch)\n",
        "      src_features = feaex(num_x, cate_x)\n",
        "      tgt_features = feaex(target_num_x, target_cate_x)\n",
        "      # Classifier feed.\n",
        "      src_pred = clf(src_features)\n",
        "      clf_loss = clf_bce(src_pred, delayed_y.to(torch.float32))\n",
        "      one_epoch_clf_train_loss.append(clf_loss.item())\n",
        "\n",
        "      src_domain_pred = dis(src_features, alpha)\n",
        "      tgt_domain_pred = dis(tgt_features, alpha)\n",
        "      # Domain loss.\n",
        "      dis_src_loss = dis_bce(src_domain_pred, D_src)\n",
        "      dis_tgt_loss = dis_bce(tgt_domain_pred, D_tgt)\n",
        "\n",
        "      dis_loss = dis_src_loss + dis_tgt_loss\n",
        "      ltot = 0.0001 * dis_loss + clf_loss\n",
        "      one_epoch_feaex_train_loss.append(dis_loss.item())\n",
        "      dis_optimizer.zero_grad()\n",
        "      clf_optimizer.zero_grad()\n",
        "      feaex_optimizer.zero_grad()\n",
        "      ltot.backward()\n",
        "      clf_optimizer.step()\n",
        "      dis_optimizer.step()\n",
        "      feaex_optimizer.step()\n",
        "\n",
        "    train_average_clf_loss_per_epoch.append(sum(one_epoch_clf_train_loss) / len(one_epoch_clf_train_loss))\n",
        "    train_average_feaex_loss_per_epoch.append(sum(one_epoch_feaex_train_loss) / len(one_epoch_feaex_train_loss))\n",
        "    print('Epoch: [{}/{}], Average CLF Loss: {:.9f}, Average Discriminantor Loss: {:.9f}'.format(epoch+1, num_epoch, train_average_clf_loss_per_epoch[-1], train_average_feaex_loss_per_epoch[-1]))\n",
        "\n",
        "    feaex.eval()\n",
        "    clf.eval()\n",
        "    pred_list = []; label_list = []\n",
        "    # Validation.\n",
        "    for (test_num_x, test_cate_x, test_y, test_delayed_y) in test_loader:\n",
        "      test_num_x = test_num_x.to(dtype=torch.float32, device=device); test_cate_x = test_cate_x.to(dtype=torch.long, device=device); test_y = test_y.to(device)\n",
        "      with torch.no_grad():\n",
        "        pred = clf(feaex(test_num_x, test_cate_x))\n",
        "      pred_list.append(pred); label_list.append(test_y)\n",
        "    pred = torch.vstack(pred_list); test_y = torch.vstack(label_list)\n",
        "    test_loss = clf_bce(pred, test_y.to(torch.float32))\n",
        "    test_loss_per_epoch.append(test_loss.item())\n",
        "    test_metric_acc, test_metric_roc_auc, test_metric_pr_auc = evaluate_model(pred, test_y)\n",
        "    metric_accuracy.append(test_metric_acc); metric_roc_auc.append(test_metric_roc_auc); metric_pr_auc.append(test_metric_pr_auc)\n",
        "    print('Test Epoch {}: test loss: {:.9f}; Accuracy: {:.9f}; ROC_AUC: {:.9f}; PR_AUC:{:.9f}'.format(epoch+1, test_loss_per_epoch[-1], test_metric_acc, test_metric_roc_auc, test_metric_pr_auc))\n",
        "    feaex.train()\n",
        "    clf.train()\n",
        "  return metric_accuracy, metric_roc_auc, metric_pr_auc"
      ],
      "metadata": {
        "id": "SPMzd4rkfdNW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initilize parameters for model and training loop.\n",
        "embed_size = 64\n",
        "input_size = 8\n",
        "output_size = 1\n",
        "clf_hidden_sizes = [32,64]\n",
        "feaex_hidden_sizes = [64,128]\n",
        "fea_dim_input = feaex_hidden_sizes[-1]\n",
        "dis_hidden_sizes = [64,64]\n",
        "cate_size = [len(cate_dict[key]) for key in cate_dict.keys()]\n",
        "dropout = 0.3\n",
        "num_epoch = 20\n",
        "batch_size = 512\n",
        "lrate = 0.001\n",
        "model_directory = ''"
      ],
      "metadata": {
        "id": "tWTvyJjGfreL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Classifier, feature extractor and discriminator\n",
        "clf = MLP(fea_dim_input, clf_hidden_sizes, output_size=output_size , predict=True, dropout = dropout).to(device)\n",
        "feaex = MLP(input_size, feaex_hidden_sizes, cate_size=cate_size, embed_size=embed_size, dropout=dropout).to(device)\n",
        "discriminator = Discriminator(fea_dim_input, dis_hidden_sizes, output_size=output_size).to(device)\n",
        "models = [clf, discriminator, feaex]\n",
        "model_names = [\"Classifier\", \"Discriminator\", \"Feature Extractor\"]\n",
        "for i, model in enumerate(models):\n",
        "    print(f'Number of parameters of {model_names[i]}: {sum(param.numel() for param in model.parameters())}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hdZUZCopkfZN",
        "outputId": "bb906fc3-72a4-442c-a2f7-68e75cbfb5bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of parameters of Classifier: 6497\n",
            "Number of parameters of Discriminator: 12737\n",
            "Number of parameters of Feature Extractor: 9800640\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Preparing Data.\n",
        "train_source_data = Criteo_Dataset(criteo_source_train_data_x, criteo_source_train_data_y, source_train_data_convertTime)\n",
        "train_source_loader = DataLoader(train_source_data, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "train_target_data = Criteo_Dataset(cirteo_target_train_data_x, criteo_target_train_data_y, target_train_data_convertTime)\n",
        "train_target_loader = DataLoader(train_target_data, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "test_data = Criteo_Dataset(criteo_test_data_x, criteo_test_data_y, test_data_convertTime)\n",
        "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)"
      ],
      "metadata": {
        "id": "jsuMPRQBUSRC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "metric_accuracy, metric_roc_auc, metric_pr_auc = train(models, lrate, num_epoch, train_source_loader, train_target_loader, test_loader, model_directory)"
      ],
      "metadata": {
        "id": "d8d9KoKXttZH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9c29361e-2062-4669-b947-34cef907b24f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: [1/20], Average CLF Loss: 0.386431146, Average Discriminantor Loss: 1.211064775\n",
            "Test Epoch 1: test loss: 0.409389973; Accuracy: 0.818116067; ROC_AUC: 0.834322387; PR_AUC:0.641663079\n",
            "Epoch: [2/20], Average CLF Loss: 0.378106685, Average Discriminantor Loss: 1.334623849\n",
            "Test Epoch 2: test loss: 0.404996723; Accuracy: 0.821445133; ROC_AUC: 0.837936970; PR_AUC:0.647843062\n",
            "Epoch: [3/20], Average CLF Loss: 0.376077274, Average Discriminantor Loss: 1.371021459\n",
            "Test Epoch 3: test loss: 0.407429725; Accuracy: 0.820804928; ROC_AUC: 0.836682318; PR_AUC:0.646237635\n",
            "Epoch: [4/20], Average CLF Loss: 0.374734568, Average Discriminantor Loss: 1.376111258\n",
            "Test Epoch 4: test loss: 0.409120530; Accuracy: 0.820410423; ROC_AUC: 0.835963088; PR_AUC:0.645177640\n",
            "Epoch: [5/20], Average CLF Loss: 0.373693655, Average Discriminantor Loss: 1.378671977\n",
            "Test Epoch 5: test loss: 0.410626590; Accuracy: 0.820514240; ROC_AUC: 0.835388657; PR_AUC:0.644675908\n",
            "Epoch: [6/20], Average CLF Loss: 0.372788212, Average Discriminantor Loss: 1.379290362\n",
            "Test Epoch 6: test loss: 0.409235567; Accuracy: 0.820669966; ROC_AUC: 0.834580013; PR_AUC:0.644137164\n",
            "Epoch: [7/20], Average CLF Loss: 0.372008950, Average Discriminantor Loss: 1.377225343\n",
            "Test Epoch 7: test loss: 0.410760581; Accuracy: 0.820611136; ROC_AUC: 0.833974535; PR_AUC:0.642875429\n",
            "Epoch: [8/20], Average CLF Loss: 0.371130363, Average Discriminantor Loss: 1.377891185\n",
            "Test Epoch 8: test loss: 0.410832345; Accuracy: 0.820431187; ROC_AUC: 0.833188326; PR_AUC:0.641645319\n",
            "Epoch: [9/20], Average CLF Loss: 0.370450300, Average Discriminantor Loss: 1.376040613\n",
            "Test Epoch 9: test loss: 0.410638928; Accuracy: 0.820230474; ROC_AUC: 0.832757770; PR_AUC:0.640615948\n",
            "Epoch: [10/20], Average CLF Loss: 0.369801075, Average Discriminantor Loss: 1.373171100\n",
            "Test Epoch 10: test loss: 0.410840124; Accuracy: 0.820334291; ROC_AUC: 0.832054007; PR_AUC:0.640007150\n",
            "Epoch: [11/20], Average CLF Loss: 0.369027661, Average Discriminantor Loss: 1.373289743\n",
            "Test Epoch 11: test loss: 0.411555767; Accuracy: 0.819894799; ROC_AUC: 0.832329476; PR_AUC:0.640236274\n",
            "Epoch: [12/20], Average CLF Loss: 0.368308043, Average Discriminantor Loss: 1.374839457\n",
            "Test Epoch 12: test loss: 0.412839949; Accuracy: 0.819870575; ROC_AUC: 0.830944284; PR_AUC:0.638087497\n",
            "Epoch: [13/20], Average CLF Loss: 0.367685381, Average Discriminantor Loss: 1.372264776\n",
            "Test Epoch 13: test loss: 0.414907932; Accuracy: 0.819842890; ROC_AUC: 0.830653373; PR_AUC:0.638094223\n",
            "Epoch: [14/20], Average CLF Loss: 0.367005632, Average Discriminantor Loss: 1.372250950\n",
            "Test Epoch 14: test loss: 0.413759261; Accuracy: 0.819566045; ROC_AUC: 0.830374657; PR_AUC:0.637586085\n",
            "Epoch: [15/20], Average CLF Loss: 0.366305614, Average Discriminantor Loss: 1.372762524\n",
            "Test Epoch 15: test loss: 0.413923174; Accuracy: 0.819559124; ROC_AUC: 0.830189388; PR_AUC:0.636196481\n",
            "Epoch: [16/20], Average CLF Loss: 0.365695772, Average Discriminantor Loss: 1.369231250\n",
            "Test Epoch 16: test loss: 0.416170895; Accuracy: 0.818953525; ROC_AUC: 0.829818239; PR_AUC:0.634126070\n",
            "Epoch: [17/20], Average CLF Loss: 0.365068919, Average Discriminantor Loss: 1.367935067\n",
            "Test Epoch 17: test loss: 0.416197777; Accuracy: 0.818808181; ROC_AUC: 0.829706214; PR_AUC:0.634020771\n",
            "Epoch: [18/20], Average CLF Loss: 0.364531640, Average Discriminantor Loss: 1.366105331\n",
            "Test Epoch 18: test loss: 0.417889059; Accuracy: 0.818846247; ROC_AUC: 0.828945487; PR_AUC:0.634419205\n",
            "Epoch: [19/20], Average CLF Loss: 0.363981857, Average Discriminantor Loss: 1.363161320\n",
            "Test Epoch 19: test loss: 0.416736841; Accuracy: 0.818514033; ROC_AUC: 0.829169935; PR_AUC:0.634308124\n",
            "Epoch: [20/20], Average CLF Loss: 0.363353297, Average Discriminantor Loss: 1.362235746\n",
            "Test Epoch 20: test loss: 0.417185724; Accuracy: 0.818354847; ROC_AUC: 0.828872850; PR_AUC:0.633308419\n"
          ]
        }
      ]
    }
  ]
}